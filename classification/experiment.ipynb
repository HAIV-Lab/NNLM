{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from model import MyModel\n",
    "from build import build_dataset\n",
    "from dataset import PrefixDataset1\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', type=float, default=0.0003, help='Learning rate')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=30, help='train epochs')\n",
    "    parser.add_argument('--milestones', type=int, nargs='+', default=[116, 233], help='Milestones')\n",
    "    parser.add_argument('--gamma', type=float, default=0.1, help='Gamma')\n",
    "    # parser.add_argument('--optimizer', type=str, default='sgd', help='optimizer')\n",
    "\n",
    "    parser.add_argument('--voc_len',type=int, default=42020, help='voc number')\n",
    "    parser.add_argument('--embedding_dim',type=int, default=1024, help='embedding size')\n",
    "    parser.add_argument('--output_dim', type=int, default=64, help=\"output dim\")\n",
    "    parser.add_argument('--dstore_mmap',type=str, default='/data/zqh/NLP/adaptive-knn-mt/store/datastore/it_finetune')\n",
    "    parser.add_argument('--dstore_size',type=int, default=3608731, help='datastore size')\n",
    "    parser.add_argument('--use_cluster', type=bool, default=True, help=\"if use word cluster\")\n",
    "    parser.add_argument('--cluster_type', type=str, default='spectrum', help='cluster type')\n",
    "    \n",
    "    # contrastive learning\n",
    "    parser.add_argument('--K', type=int, default=200, help='queue size')\n",
    "    parser.add_argument('--m', type=float, default=0.999, help='momentum')\n",
    "    parser.add_argument('--class_num', type=int, default=42020, help=\"class number\")\n",
    "    \n",
    "\n",
    "    # save\n",
    "    parser.add_argument('--save_path', type=str, default='/data/zqh/adaptive-knn-mt/checkpoints/koran', help='save checkpoint dir')\n",
    "    # dataset\n",
    "    args = parser.parse_args([])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "dataset= PrefixDataset1(args=args)\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "\n",
    "choice_label = np.arange(args.voc_len)\n",
    "\n",
    "new_keys = None\n",
    "new_values = None\n",
    "\n",
    "for i in choice_label:\n",
    "    embedding = dataset.data[dataset.label==i]\n",
    "    labels = dataset.label[dataset.label==i]\n",
    "\n",
    "    # 少于某个数的时候保留全部的key-value对\n",
    "    if len(embedding) <= 1000:\n",
    "        if new_keys is None:\n",
    "            new_keys = embedding\n",
    "            new_values = labels\n",
    "        else:\n",
    "            new_keys = np.concatenate((new_keys, embedding))\n",
    "            new_values = np.concatenate((new_values, labels))\n",
    "        continue\n",
    "\n",
    "    # sc = SpectralClustering(n_clusters=8, affinity='nearest_neighbors', n_init=3, verbose=3)\n",
    "    sc = KMeans(n_clusters=3)\n",
    "    sc.n_clusters = int(math.log(len(embedding))) + 1\n",
    "    \n",
    "    sc.fit(embedding)\n",
    "    cluster_label = sc.predict(embedding)\n",
    "\n",
    "    cls_embedding = None\n",
    "    cls_labels = None\n",
    "    for cls_label in np.unique(cluster_label):\n",
    "        temp_embedding = embedding[cluster_label==cls_label]\n",
    "        temp_labels = labels[cluster_label==cls_label]\n",
    "        select_index = np.arange(len(temp_embedding))\n",
    "        np.random.shuffle(select_index)\n",
    "        select_index = select_index[:int(0.3 * len(temp_embedding))]\n",
    "\n",
    "\n",
    "        if cls_embedding is None:\n",
    "            cls_embedding = temp_embedding[select_index]\n",
    "            cls_labels = temp_labels[select_index]\n",
    "        else:\n",
    "            cls_embedding = np.concatenate((cls_embedding, temp_embedding[select_index]))\n",
    "            cls_labels = np.concatenate((cls_labels, temp_labels[select_index]))\n",
    "        \n",
    "    if new_keys is None:\n",
    "        new_keys = np.array(cls_embedding, dtype=np.float16)\n",
    "        new_values = np.array(cls_labels, dtype=np.int)\n",
    "    else:\n",
    "        new_keys = np.concatenate((new_keys, np.array(cls_embedding, dtype=np.float16)))\n",
    "        new_values = np.concatenate((new_values, np.array(cls_labels, dtype=np.int)))\n",
    "\n",
    "dstore_key = np.memmap(\"./new_kyes.npy\", dtype=np.float16, mode=\"w+\",\n",
    "shape=new_keys.shape)\n",
    "dstore_values = np.memmap(\"./new_values.npy\", dtype=np.int, mode=\"w+\",\n",
    "shape=new_values.shape)\n",
    "\n",
    "dstore_key[:,:] = new_keys\n",
    "dstore_values[:] = new_values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys.shape\n",
    "# IT domain 59132 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embedding = embedding\n",
    "src_labels = labels\n",
    "\n",
    "embedding = np.array(embedding[cluster_label!=-1])\n",
    "cluster_label = np.array(cluster_label[cluster_label!=-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "dataset= PrefixDataset1(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vals = [0 for i in range(args.voc_len)]\n",
    "for i in dataset.label:\n",
    "    frequency_vals[i] += 1\n",
    "\n",
    "plt.plot(np.arange(100, args.voc_len), frequency_vals[100:])\n",
    "plt.show()\n",
    "print(np.float32(sum(frequency_vals)/args.voc_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "tsne_embedding = tsne.fit_transform(embedding)\n",
    "\n",
    "plt.scatter(tsne_embedding[:,0], \n",
    "tsne_embedding[:,1], c=cluster_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_embedding[:,0], \n",
    "tsne_embedding[:,1], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.data = embedding\n",
    "        self.labels = cluster_label\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        embedding = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        return embedding, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mymodel = MyModel(args).cuda()\n",
    "\n",
    "dataset = EmbeddingDataset(args)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset = dataset,\n",
    "    batch_size = args.batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(mymodel.parameters(), args.lr, \n",
    "                                         momentum=0.9, nesterov=True,\n",
    "                                         weight_decay=0.0004)\n",
    "# self.optimizer = optim.FP16Optimizer.build_optimizer(self.args, params)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.milestones, gamma=args.gamma)\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    correct = 0\n",
    "    data_len = 0\n",
    "    for i, (x, label) in enumerate(dataloader):\n",
    "        x = x.cuda()\n",
    "        label = label.cuda().long()\n",
    "\n",
    "        logits, loss = mymodel.fc_encode(x, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predictions = logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            batch_correct = predictions.eq(label.view_as(predictions)).sum().item()\n",
    "            acc = batch_correct / x.shape[0]\n",
    "\n",
    "            print(f\"Train epoch: {epoch} loss: {loss} acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_label = np.array(cluster_label)\n",
    "# index = np.where(cluster_label==8)[0]\n",
    "# temp_labels = labels[index]\n",
    "# print(\"cluster embedding nums:\", len(temp_labels))\n",
    "# print(np.unique(temp_labels))\n",
    "\n",
    "cluster_embedding_num = {}\n",
    "for c_label in cluster_label:\n",
    "    cluster_embedding_num[c_label]=[]\n",
    "    for i in range(args.voc_len):\n",
    "        cluster_embedding_num[c_label].append(0)\n",
    "    index = np.where(cluster_label==c_label)[0]\n",
    "    label = labels[index]\n",
    "    for i in label:\n",
    "        cluster_embedding_num[c_label][i] += 1.0\n",
    "    cluster_embedding_num[c_label] = np.array(cluster_embedding_num[c_label])\n",
    "    cluster_embedding_num[c_label] = cluster_embedding_num[c_label]/np.float32(len(label))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import unique\n",
    "\n",
    "\n",
    "print(embedding.shape)\n",
    "print(cluster_label.shape)\n",
    "for i in cluster_label:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=8)\n",
    "KNN.fit(src_embedding, src_labels)\n",
    "\n",
    "result = KNN.predict(embedding[0:1])\n",
    "result_prob = KNN.predict_proba(embedding[0:1])\n",
    "print(result)\n",
    "print(result_prob)\n",
    "print(result_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_embedding = torch.tensor(embedding[0:1]).cuda()\n",
    "temp_label = torch.tensor(cluster_label[0:1]).cuda().long()\n",
    "result, loss = mymodel.fc_encode(temp_embedding, temp_label)\n",
    "print(result)\n",
    "predictions = result.argmax(dim=-1, keepdim=True)\n",
    "for i in cluster_embedding_num[predictions[0][0].item()]:\n",
    "    if i !=0:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('adaptive-knn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a486ca640bf467f84ac38103d0adf31508c8d9bebf3e22116d5da9069abbb3a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
